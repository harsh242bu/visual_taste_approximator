# Visual Taste Approximator (VTA)

Visual taste approximator is a very simple tool that helps anyone create an automatic replica of themselves, one that can approximate their own personal visual taste.  
This is done by first labeling a few images and then training a machine learning model to mimic ourselves.  

## Environment
Follow the steps below to create a suitable conda environment for VTA
conda create -n vta anaconda python=3.10.13
conda activate vta
pip install numpy
pip install lightgbm
pip install matplotlib
pip install -U scikit-learn
python -m pip install scipy
##Install clip from the [clip repository](https://github.com/harsh242bu/CLIP.git) by running following command:
python setup.py install

If you encounter any library error after going through the above steps then just install the required libraries till the error resolves

## For training a classifier
To train a VTA classifier the first step is to copy the images generated to respective positively_labeled and negatively_labeled folder. For example in our case we are training a classifier to filter out proper portrait images out of all the images generated by stable diffusion. In order to do so, we will manually annotate around a 100 images into two categories, proper high quality portrait and rest of the other. After annotating copy the images to positively_labeled and negatively_labeled folders respectively. After pasting the images run the following command

python train_binary_classfier.py --positive_folder positively_labeled --negative_folder negatively_labeled --model_name face_quality_classifier

If you run into issues with any of the basic ML algorithms like KNN, LightGBM or Logistic Regression, modify the arguments for that particular algorithm you are facing issue with in train_binary_classfier.py file (from line 56-86).

optional arguments:
  -h, --help            show this help message and exit
  --test_folder TEST_FOLDER
                        path to folder to classify
  --positive_folder POSITIVE_FOLDER
                        path to folder with positively labeled images
  --negative_folder NEGATIVE_FOLDER
                        path to folder with negatively labeled images
  --models_folder MODELS_FOLDER
                        path to where to store the resulting trained models
  --model_name MODEL_NAME
                        the prefix name of the model that will be saved
  --num_features_sets {1,2,4,6}
                        number of pretrained features sets to use (more features takes longer but more accurate)

## For testing the classifier
To test or use the classifier on the images you generated, copy paste the images in test_folder. Then run the following command to classify the images.

python use_classifier_on_test_folder.py --test_folder test_folder --model_path models_folder/face_quality_classifier_num_samples_9680_num_features_5760_2023-12-19.pickle

This will create two new folders named `likely_positive/` and `likely_negative/`. Under likely_positive you'll have "proper" portrait images. The name of the model for model_path argument might be a little different for you so use that.

optional arguments:
--test_folder TEST_FOLDER
                        path to folder to classify
--model_path MODEL_PATH
					path to pretrained model to use for classification
--positive_out_folder POSITIVE_OUT_FOLDER
					path to positive output folder images
--negative_out_folder NEGATIVE_OUT_FOLDER
                    path to negative output folder images
--positive_threshold POSITIVE_THRESHOLD
                    threshold above which all images in src 'test_folder' will be moved to the 'positive_out_folder'
--negative_threshold NEGATIVE_THRESHOLD
                    threshold below which all images in src 'test_folder' will be moved to the 'negative_out_folder'
--delete_src
                    if enabled, deletes files in src 'test_folder' that are transferred to output positive and negative folders


### Using the GUI (not tested)
For any new task of your choosing, one can run the labeling GUI to define it
```
python image_labeling_GUI.py --test_folder TEST_FOLDER --positive_folder POSITIVE_FOLDER --negative_folder NEGATIVE_FOLDER --model_name MODEL_NAME
```

- An image will be presented at the center of the screen
    - by pressing the "positive sample" button (or "3" on your keyboard) you can label it as positive. this sample will be transfered to the positive folder,
    - by pressing the "nagative sample" button (or "1" on your keyboard) it will be labeled as negative and tranfered to negative folder.
    - pressing "next image" (or "2") will skip to next image without making a decision
- After labeling some number of images (I suggest at least 50 positive and 50 negatives to start, this should not take more than a few minutes)
    - one can press the "train/re-train classifier" button (or "t" button on keyboard)
    - this will train a classifier on all images in the positive vs negative folders and the graphs on the right will be updated. this classifier will be saved in the models_folder with the requested prefix model_name
    - the top most graph on the right will display the histogram of classifier prediction on internal cross vlaidation of positives (green) and negatives (red)
- One can use these histograms to zoom in on areas where the classifier is not certain (where histogram overlap). this is done by the left and right sliders at the buttom right of the screen.
    - during labeling, images from the folder to classify will be presented mostly from inside the selected ranges using the sliders
- If the positive and negative folder are not empty in the begining, then a classifier will first be trained on them and one can continue to labeling images

If you wish to tradeoff speed vs accuracy of the approximation, you can use more or less pretrained feature sets by using the --num_features_sets option (supported options are {1,2,4,7})
the option with a single feature set will use only CLIP ViT-L/14 @336 model and will run approximatly 8 times faster than using the full 7 feature sets (multiple CLIP models, ConvNext_XL, multiple DINO models, and BEiT), but it will be less accurate.  
The specific accuracy details will depend on the specific task, but the differece can be quite substantial, in particular when having a small amoung of labeled images

## Miscellaneous
If you want to specify a directory for the models to download, you can specify them using the following command

export HF_HOME=<path to huggingface home directory>
export HF_DATASETS_CACHE=<path to huggingface cache directory>
export TORCH_HOME=<path to torch home>
